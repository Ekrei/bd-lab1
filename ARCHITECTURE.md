# Архитектура проекта: как всё связано и почему выбраны эти инструменты

Документ описывает: **роль каждого файла**, **порядок выполнения этапов**, **связи между модулями** и **обоснование выбора технологий**.

---

## 1. Общая схема потока данных

```
[Steam] [GOG] [Epic]  ──парсеры──►  data/raw/*.json
                                            │
                                            ▼
[PostgreSQL]  ◄── load_raw_to_db  ◄──  (сырые JSON)
     │
     ├── run_schema  (создание таблиц)
     │
     ▼
[products, offers, attributes]  ◄──  deduplicate  (объединение дубликатов)
     │
     ▼
[FastAPI + Jinja2]  ◄── app/db.get_conn()  ◄──  поиск и отображение
     │
     ▼
  Пользователь (браузер)
```

**Последовательность при первом запуске:**
1. `run_parsers.py` → собирает данные в `data/raw/`
2. `sql/001_schema.sql` → создаёт таблицы (вручную через psql или через `run_schema.py`)
3. `load_raw_to_db.py` → заливает JSON в БД
4. `deduplicate.py` → склеивает дубликаты в `products`
5. `run_app.py` / uvicorn → поднимает веб‑приложение, которое читает из БД

---

## 2. Структура каталогов и роль файлов

### 2.1. Корень проекта

| Файл | Назначение |
|------|------------|
| `requirements.txt` | Список зависимостей Python. Все скрипты и приложение ориентированы на эти библиотеки. |
| `run_parsers.py` | **Точка входа этапа 1.** Импортирует парсеры из `parsers/`, вызывает `fetch_all(limit=1100)` у каждого, сохраняет результат в `data/raw/{steam,gog,epic}_raw.json`. |
| `run_app.py` | **Точка входа веб‑приложения.** Запускает uvicorn с `app.main:app` (reload для разработки). |
| `test_parsers.py` | Упрощённая проверка парсеров с малым лимитом (3–5). Не входит в основной пайплайн. |
| `.env.example` | Пример переменных (`DATABASE_URL`). Реальные значения в `.env` (не коммитятся). |
| `Dockerfile` | Образ приложения: Python, установка `requirements.txt`, копирование `app`, `sql`, `scripts`, `run_app.py`. При старте: `run_schema.py` → uvicorn. |
| `docker-compose.yml` | Сервисы `postgres` и `app`. `app` зависит от `postgres` (healthcheck); `DATABASE_URL` указывает на контейнер `postgres`. |
| `.dockerignore` | Исключает `data/`, `.env`, `__pycache__` и т.п. при сборке образа, чтобы не тащить лишнее и сырые данные. |

---

### 2.2. `parsers/` — сбор данных с площадок

**Идея:** три разных источника (Steam, GOG, Epic) приводятся к **одной структуре** `CatalogItem`, чтобы дальше всё обрабатывать одинаково.

| Файл | Роль |
|------|------|
| `base.py` | **Общий контракт.** `CatalogItem` — dataclass с полями: `source`, `source_id`, `title`, `url`, `description`, `price`, `release_year`, `platforms`, `developers`, `genres` и т.д. Метод `to_dict()` нужен для сериализации в JSON. `BaseParser` — абстрактный класс с `fetch_all(limit)`. Все парсеры наследуют `BaseParser` и возвращают `list[CatalogItem]`. |
| `steam.py` | Парсер Steam. Вызывает **официальный API**: `GetAppList/v2` (список appid) и `store.steampowered.com/api/appdetails?appids=...` по каждому appid. Оставляет только `type == "game"`. |
| `gog.py` | Парсер GOG. Использует **публичный каталог** `catalog.gog.com/v1/catalog` с пагинацией (`page`, `perPage`). Фильтр: `productType == "game"`. |
| `epic.py` | Парсер Epic. Сначала **GraphQL** `www.epicgames.com/graphql` (persisted query `searchStoreQuery`) с пагинацией; при нехватке — **Playwright** (скролл по `store.epicgames.com/.../browse` и разбор карточек в DOM). |

**Связи:**
- `run_parsers.py` импортирует `SteamParser`, `GOGParser`, `EpicParser` и вызывает у каждого `fetch_all()`.
- Каждый парсер импортирует `BaseParser` и `CatalogItem` из `base.py`.
- Результат пишется в JSON; схема соответствует `CatalogItem.to_dict()`.

---

### 2.3. `data/raw/` — сырые данные

Каталог создаётся `run_parsers.py`. Файлы:
- `steam_raw.json`, `gog_raw.json`, `epic_raw.json` — массивы объектов в формате `to_dict()`.

Их читает только `load_raw_to_db.py`. В Docker этот каталог монтируется с хоста (`-v .../data:/app/data`), чтобы не закладывать большие JSON в образ.

---

### 2.4. `sql/` — схема БД

| Файл | Роль |
|------|------|
| `001_schema.sql` | `CREATE TABLE` для `products`, `offers`, `attributes` и `CREATE INDEX`. Используется: вручную (`psql -f`), при старте контейнера — через `run_schema.py`, который выполняет выражения из файла по одному (разбивка по `;`). |

**Связи:**
- `offers.product_id` → `products.id`
- `attributes.product_id` → `products.id`
- `run_schema.py` открывает `sql/001_schema.sql` по пути `PROJECT_ROOT/sql/001_schema.sql`.

---

### 2.5. `scripts/` — загрузка, схема, дедупликация

| Файл | Роль |
|------|------|
| `run_schema.py` | Подключается к `DATABASE_URL`, читает `sql/001_schema.sql`, разбивает по `;`, выполняет каждое непустое и не‑комментарий выражение. Вызывается: вручную, из `Dockerfile` перед uvicorn. |
| `load_raw_to_db.py` | Читает `data/raw/steam_raw.json`, `gog_raw.json`, `epic_raw.json`. Для каждой записи: проверяет, есть ли уже `(website_name, source_id)` в `offers`; если нет — создаёт строку в `products`, одну в `offers`, нужное число в `attributes` (platform, genre, developer, publisher, rating). Использует `psycopg2` и `execute_values` для пачек `attributes`. |
| `deduplicate.py` | Загружает все `products` (id, canonical_name, release_year), платформы из `attributes` и кол‑во offers. Разбивает по году на «бакеты», внутри каждого ищет кластеры дубликатов по: нормализованное/«fuzzy» название (rapidfuzz ≥88%), год, пересечение платформ. В каждом кластере один продукт — «победитель» (больше offers). Остальные: `UPDATE offers` → `product_id` победителя, копирование `attributes` (без дублей), `DELETE` старых продуктов и их `attributes`. |

**Связи:**
- `load_raw_to_db` зависит от наличия `data/raw/*.json` и от применённой схемы (`run_schema` или `psql -f`).
- `deduplicate` — от уже загруженных `products`, `offers`, `attributes` (т.е. после `load_raw_to_db`).
- Все три скрипта берут `DATABASE_URL` из окружения (через `dotenv` при наличии).

---

### 2.6. `app/` — веб‑приложение

| Файл | Роль |
|------|------|
| `db.py` | Функция `get_conn()`: `psycopg2.connect(DATABASE_URL, cursor_factory=RealDictCursor)`. Единственная точка подключения к БД для приложения. |
| `main.py` | FastAPI‑приложение. Роуты: `GET /api/search?q=`, `GET /api/product/{id}` (JSON), `GET /` (главная с поиском), `GET /product/{id}` (страница товара). Функции `_search(q)` и `_product(id)` вызывают `get_conn()`, выполняют SQL, возвращают списки/словари. Шаблоны — Jinja2 из `templates/`. |
| `templates/base.html` | Базовый HTML (header, блок `content`). |
| `templates/index.html` | Форма поиска, при переданном `q` — вывод `results` (карточки: название, картинка, год, мин. цена, ссылка на `/product/<id>`). |
| `templates/product.html` | Карточка товара: название, год, атрибуты, описание, блок «Где купить» (offers: сайт, цена, ссылка). |
| `static/style.css` | Стили для сетки карточек, страницы товара, формы поиска. |
| `static/placeholder.svg` | Заглушка, если у товара нет `image_url`. |

**Связи:**
- `main.py` импортирует `get_conn` из `app.db`.
- `main.py` монтирует `StaticFiles` на `/static` и использует `Jinja2Templates(directory="templates")`.
- Шаблоны расширяют `base.html` и получают `request`, `q`, `results` / `product`, `offers`, `attributes` из `main.py`.

---

## 3. Почему выбран именно этот инструмент (а не другой)

### 3.1. Парсинг

| Вопрос | Выбор | Причина |
|--------|-------|---------|
| **Steam: API или парсинг HTML?** | Официальный API (`GetAppList`, `appdetails`). | API стабильнее, есть типы (игра/программа/DLC), не ломается при смене вёрстки. Парсинг HTML потребовал бы обхода антибот‑механизмов и давал бы хрупкий код. |
| **GOG: API или скрейпинг?** | Публичный каталог `catalog.gog.com/v1/catalog`. | У GOG есть удобный JSON‑каталог с пагинацией, ценой, жанрами, датами. Парсинг HTML не даёт преимуществ и сложнее в поддержке. |
| **Epic: только GraphQL или только браузер?** | Сначала GraphQL (`searchStoreQuery`), при нехватке — Playwright. | Публичного каталога у Epic нет; GraphQL — неофициально, но даёт структурированный ответ. Когда ответ пустой или блокируется, Playwright получает хотя бы название, ссылку, картинку со страницы каталога и позволяет выйти на 1000+ записей. |
| **Playwright, а не Selenium?** | Playwright. | Меньше зависимостей, простой `headless=True`, стабильный API. Selenium чаще используется для legacy‑проектов. |
| **`requests` + `BeautifulSoup` в requirements** | `requests` — да, `BeautifulSoup` — по необходимости. | `requests` — для всех HTTP‑запросов (Steam, GOG, Epic GraphQL). BeautifulSoup оставлен на случай, если понадобится разбор HTML; в текущих парсерах не используется. |

---

### 3.2. База данных

| Вопрос | Выбор | Причина |
|--------|-------|---------|
| **PostgreSQL, а не MySQL/SQLite?** | PostgreSQL. | В задании указан пример «PostgreSQL или MySQL». PostgreSQL удобен для JSON, индексов, полнотекстового поиска (на будущее). SQLite проще, но для «интеграционной» БД и возможного роста объёма PostgreSQL предпочтительнее. |
| **Чистый SQL в `001_schema.sql`, а не ORM‑миграции?** | Один файл с `CREATE TABLE` и `CREATE INDEX`. | Проект учебный; схема небольшая. Отдельные миграции (Alembic и т.п.) добавляют сложность без выгоды на текущем масштабе. Файл можно применить и через `psql`, и через `run_schema.py`. |
| **`psycopg2`, а не `asyncpg` или SQLAlchemy?** | `psycopg2` (и `psycopg2-binary` для простой установки). | Скрипты загрузки и дедупликации — синхронные; FastAPI в данном проекте тоже не использует async для БД. `psycopg2` хорошо документирован, `RealDictCursor` упрощает работу с выборками. SQLAlchemy имеется в requirements на перспективу, но в коде пока не используется, чтобы не усложнять. |

---

### 3.3. Дедупликация

| Вопрос | Выбор | Причина |
|--------|-------|---------|
| **`rapidfuzz`, а не `fuzzystring` / `fuzzywuzzy`?** | `rapidfuzz`. | `rapidfuzz` — форк/развитие `fuzzywuzzy`, быстрее и поддерживается. `fuzzystring` менее распространён. |
| **Порог 88%** | Эмпирически подобран. | Ниже — больше ложных склеек («Game» и «Game 2»); выше — пропуск вариаций вроде «The Witcher 3» / «The Witcher 3: Wild Hunt». 88% — компромисс; при необходимости настраивается константой в `deduplicate.py`. |
| **Критерии: название + год + платформы** | Как в задании. | Для игр нет единого кода (как ISBN у книг). Связка «нормализованное/похожее название + один год + пересечение платформ» даёт разумный баланс между слиянием дубликатов и разделением разных игр. |
| **Бакеты по году внутри дедупликации** | Да. | Сравнивать все пары продуктов — O(n²). Группировка по году сильно сокращает число сравнений при сохранении корректности (год должен совпадать). |

---

### 3.4. Веб‑приложение

| Вопрос | Выбор | Причина |
|--------|-------|---------|
| **FastAPI, а не Flask/Django?** | FastAPI. | Автодокументация (OpenAPI), встроенная валидация, асинхронность при необходимости. Flask проще, но с ростом API FastAPI удобнее. Django — избыточен для «минимального» приложения с двумя эндпоинтами и двумя страницами. |
| **Jinja2 в шаблонах, а не SPA (React/Vue)?** | Jinja2, серверный рендеринг. | В задании — «простой фронтенд». Поиск и карточка товара реализуются формами и ссылками; дополнительный JS‑фреймворк не требуется. Jinja2 входит в экосистему FastAPI/Starlette, шаблоны легко читать и менять. |
| **Отдельные маршруты `/` и `/api/search`** | Да. | `/` отдаёт HTML с результатами при переданном `q`; `/api/search?q=` — JSON для возможного AJAX или внешних клиентов. Дублирование логики `_search()` невелико, зато явно разделены «страница» и «API». |

---

### 3.5. Docker

| Вопрос | Выбор | Причина |
|--------|-------|---------|
| **Один образ для приложения, без парсеров в образе** | В образ копируются `app`, `sql`, `scripts`, `run_app.py`; парсеры и `data/` — нет. | Парсеры нужны на этапе сбора; для работы сайта достаточно БД и приложения. `data/` не копируется, чтобы не раздувать образ и не хранить сырые данные в образе; при необходимости монтируется с хоста. |
| **`run_schema` в CMD перед uvicorn** | Да. | При первом старте контейнера БД пустая; `CREATE TABLE IF NOT EXISTS` и индексы применяются один раз. Повторный запуск не ломает уже существующие таблицы. |
| **`depends_on` с `condition: service_healthy` для postgres** | Да. | Приложение не стартует, пока `pg_isready` не вернёт успех. Это уменьшает ошибки подключения при одновременном `docker-compose up`. |

---

## 4. Зависимости между этапами (кратко)

- **Парсинг** не зависит ни от чего, кроме интернета и `data/raw/` (создаётся при первом запуске).
- **Схема БД** — отдельный шаг; выполняется до первой загрузки.
- **Загрузка** (`load_raw_to_db`) требует: `data/raw/*.json`, применённую схему, доступную PostgreSQL.
- **Дедупликация** — после загрузки; меняет только `products`, `offers`, `attributes`.
- **Веб‑приложение** — после применения схемы; для наполненного каталога нужны загрузка и дедупликация. Работает и с пустой БД (пустой поиск).

---

## 5. Где и что править при доработках

- **Добавить источник (например, Xbox):** новый файл `parsers/xbox.py` (наследник `BaseParser`), подключение в `run_parsers.py`, при загрузке — новый JSON и ветка в `load_raw_to_db.py` (тот же набор полей `CatalogItem`).
- **Поменять критерии дедупликации:** константы и логика в `scripts/deduplicate.py` (`_find_clusters`, `_platforms_overlap`, порог rapidfuzz).
- **Новые поля продукта:** правки в `sql/001_schema.sql` (миграция или новый файл), в `load_raw_to_db.py` (маппинг из JSON), при необходимости — в `app/templates/product.html` и `_product()` в `main.py`.
- **Другой поиск (full‑text и т.п.):** изменение SQL в `_search()` в `app/main.py` и, при необходимости, индексов в `001_schema.sql`.
